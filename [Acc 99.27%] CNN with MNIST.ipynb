{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN application to detect digits database MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset consisting thousands of handwritten digits. In this Jupyter Notebook, we are going to use TensorFlow in building Convolutional Neural Networks in order to correctly identify the input image: '0', '1', '2', ..., '8', or '9'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "import sys\n",
    "import os\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "drop_out = 0.5\n",
    "epoch_num = 20\n",
    "width, height = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is available on YanLecun website at `http://yann.lecun.com/exdb/mnist/`. There are four files for training and test data. There are also similar dataset on Kaggle website in the form of csv but I have decided to use the ones from the original source.\n",
    "\n",
    "First, let's load our raw data which is saved in folder `data`. To make things easier, I already extracted the originals file and keep them in `data` folder. Then I adapted the code for loading them from the book for Python. As we can see that there are about 60k rows and 784 columns in training set and 10k rows for testing. The dimension of every image is of size 28 x 28 (which is 784 pixels in total). Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, the higher the number the darker the pixel is and those values are integers ranging between 0 and 255.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "Below is an example of a data row:\n",
    "\n",
    "```java\n",
    "  0   0   0   0 ...   0   0\n",
    "247 127   0   0 ...   0   0\n",
    "254 254 254 254 ... 082 083\n",
    " |   |   |   |  ...  |   |\n",
    " 73  89  89  93 ...  18 232\n",
    "  0   0   0   0 ...    0  0 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from Chapter 15 of the book Python Machine Learning (Second Edition)\n",
    "def load_mnist(path, kind=\"train\"):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\"%s-labels-idx1-ubyte\"% kind)\n",
    "    images_path = os.path.join(path,\"%s-images-idx3-ubyte\"% kind)\n",
    "\n",
    "    with open(labels_path, \"rb\") as lbpath:\n",
    "        magic, n = struct.unpack(\">II\", lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, \"rb\") as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784),  Train labels shape: (60000,)\n",
      "Test shape:  (10000, 784),  Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "raw_data, raw_labels = load_mnist(\"./data\", kind=\"train\")\n",
    "print(\"Train shape: {},  Train labels shape: {}\".format(raw_data.shape, raw_labels.shape))\n",
    "raw_test_data, raw_test_labels = load_mnist(\"./data\", kind=\"t10k\")\n",
    "print(\"Test shape:  {},  Test labels shape:  {}\".format(raw_test_data.shape, raw_test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for row 10th: \n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  42 118 219\n",
      " 166 118 118   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 103 242 254 254 254 254 254  66   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18 232\n",
      " 254 254 254 254 254 238  70   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0 104 244 254 224 254 254 254 141   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 207 254 210 254 254 254  34   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  84 206 254 254 254 254\n",
      "  41   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  24 209 254 254 254 171   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  91 137 253 254\n",
      " 254 254 112   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  40 214 250 254 254 254 254 254  34   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  81 247 254 254\n",
      " 254 254 254 254 146   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 110 246 254 254 254 254 254 171   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  73  89  89  93 240 254 171   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   1 128 254 219  31\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   7 254 254 214  28   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 138 254 254\n",
      " 116   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  19 177\n",
      "  90   0   0   0   0   0  25 240 254 254  34   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0 164 254 215  63  36   0  51  89 206 254\n",
      " 254 139   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  57 197 254 254 222 180 241 254 254 253 213  11   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 140 105 254 254 254 254 254\n",
      " 254 236   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   7 117 117 165 254 254 239  50   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data for row 10th: \\n{}\".format(raw_data[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 labels in train data: \n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 labels in train data: \\n{}\".format(raw_labels[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get all pixel data and save it into an array. As all of the pixel values are from 0 to 255, we will convert them by dividing each by `255` with the help of function <span style=\"color:blue; font-family:Courier\">convert_data</span>.\n",
    "\n",
    "Please also note the datatype here is `np.float32`, later when we create TensorFlow Graph, datatype for placeholder training input `x` must be declared in the same type `tf.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data(data):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data: raw data\n",
    "    Return:\n",
    "        array in the form of float values\n",
    "    '''\n",
    "    img = data.astype(np.float32)\n",
    "    return np.multiply(img, 1.0 / 255.0) # Convert from [0:255] to [0.0:1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting raw train data...\n",
      "Converting raw test data...\n",
      "Done converting...\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting raw train data...\")\n",
    "raw_converted_train_data = convert_data(raw_data)\n",
    "print(\"Converting raw test data...\")\n",
    "raw_converted_test_data = convert_data(raw_test_data)\n",
    "print(\"Done converting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for the shape of train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image shape: (60000, 784)\n",
      "Test Image shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Image shape: {}\".format(raw_converted_train_data.shape))\n",
    "print(\"Test Image shape: {}\".format(raw_converted_test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the value for our first image after convertion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image: \n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01176471\n",
      "  0.07058824  0.07058824  0.07058824  0.49411768  0.53333336  0.68627453\n",
      "  0.10196079  0.65098041  1.          0.96862751  0.49803925  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.11764707  0.14117648  0.36862746\n",
      "  0.60392159  0.66666669  0.99215692  0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.88235301  0.67450982  0.99215692  0.94901967  0.76470596\n",
      "  0.25098041  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.19215688\n",
      "  0.9333334   0.99215692  0.99215692  0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.99215692  0.99215692  0.98431379  0.36470589  0.32156864\n",
      "  0.32156864  0.21960786  0.15294118  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.07058824  0.8588236   0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.99215692  0.77647066  0.71372551  0.96862751  0.9450981   0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.3137255   0.61176473  0.41960788\n",
      "  0.99215692  0.99215692  0.80392164  0.04313726  0.          0.16862746\n",
      "  0.60392159  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.05490196  0.00392157  0.60392159  0.99215692  0.35294119  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.54509807  0.99215692  0.74509805  0.00784314\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.04313726  0.74509805  0.99215692\n",
      "  0.27450982  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.13725491\n",
      "  0.9450981   0.88235301  0.627451    0.42352945  0.00392157  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.31764707  0.94117653  0.99215692  0.99215692  0.4666667   0.09803922\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.17647059  0.72941178  0.99215692  0.99215692\n",
      "  0.58823532  0.10588236  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.0627451   0.36470589\n",
      "  0.98823535  0.99215692  0.73333335  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.97647065  0.99215692  0.97647065  0.25098041  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.18039216  0.50980395\n",
      "  0.71764708  0.99215692  0.99215692  0.81176478  0.00784314  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.15294118  0.58039218  0.89803928\n",
      "  0.99215692  0.99215692  0.99215692  0.98039222  0.71372551  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.09411766  0.44705886  0.86666673  0.99215692\n",
      "  0.99215692  0.99215692  0.99215692  0.78823537  0.30588236  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.09019608  0.25882354  0.83529419  0.99215692  0.99215692\n",
      "  0.99215692  0.99215692  0.77647066  0.31764707  0.00784314  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.07058824  0.67058825  0.8588236   0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.76470596  0.3137255   0.03529412  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.21568629  0.67450982  0.88627458  0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.95686281  0.52156866  0.04313726  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.53333336  0.99215692  0.99215692  0.99215692  0.83137262\n",
      "  0.52941179  0.51764709  0.0627451   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"First image: \\n{}\".format(raw_converted_train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define <span style=\"color:blue; font-family:Courier\">display_image</span> function to show some images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_image(data, width, height):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data: data image\n",
    "        width: the width of display image (which is 28)\n",
    "        height: the height of display image (which is also 28)\n",
    "    Return:\n",
    "        plot containing 20 images of our first 20 digits\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    for i in range(1, 21):\n",
    "        fig.add_subplot(4, 5, i)\n",
    "        new_image = data[i-1].reshape(width, height)\n",
    "        plt.imshow(new_image, cmap=cm.binary)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first 20 digits in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAGQCAYAAADycFR6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmBc+dd1wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10850d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(raw_converted_train_data,width,height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check to see how many classes are there for label in our dataset. As expected, there are 10 different labels which corresponding to our 10 digits from `0` to `9`. The first 20 digits match with our image plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels: (60000,)\n",
      "Count for labels: 10\n",
      "First 20 labels:[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]\n"
     ]
    }
   ],
   "source": [
    "labels_count = np.unique(raw_labels)\n",
    "print(\"Shape of labels: {}\".format(raw_labels.shape))\n",
    "print('Count for labels: {0}'.format(labels_count.shape[0]))\n",
    "print(\"First 20 labels:{}\".format(raw_labels[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting labels data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining <span style=\"color:blue; font-family:Courier\">convert_to_one_hot</span> function to convert labels values into the format of one-hot vectors. \n",
    "\n",
    "For example:\n",
    "    \n",
    "    Digit 0: [1 0 0 0 0 0 0 0 0 0]\n",
    "    Digit 3: [0 0 0 1 0 0 0 0 0 0]\n",
    "\n",
    "- We can also use pre-defined function `tf.one_hot` from TensorFlow to convert all labels into one-hot vectors. If you decide to use this method, when creating the graph, the shape for placeholder of labels `y` should be `[None]` as sample code below:\n",
    "\n",
    "    ```python\n",
    "    tf.one_hot(indices=tf_y, \n",
    "               depth=10,\n",
    "               dtype=tf.float32,\n",
    "               name=\"tf_y_onehot\")\n",
    "\n",
    "    tf_y = tf.placeholder(tf.int32, \n",
    "                          shape=[None],\n",
    "                          name='tf_y')\n",
    "    ```\n",
    "\n",
    "- Or using the function <span style=\"color:blue; font-family:Courier\">convert_to_one_hot</span> and `[None, 10]` for the labels `y` in Tensor graph. Here, we prefer this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(labels_data, max_length):\n",
    "    '''\n",
    "    Arguments:\n",
    "        labels_data: our raw data containing labels values\n",
    "        max_length: the maximum columns of our one-hot vectors (class numbers)\n",
    "    Return: \n",
    "        Labels in the form of one-hot vectors\n",
    "    '''\n",
    "    label_binarizer = pre.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max_length))\n",
    "    one_hot_label = label_binarizer.transform(labels_data)\n",
    "    one_hot_label = one_hot_label.astype(np.int32)\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train labels: (60000, 10)\n",
      "First 5 train labels: \n",
      "[[0 0 0 0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "Shape of test labels: (10000, 10)\n",
      "First 5 test labels: \n",
      "[[0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = convert_to_one_hot(raw_labels,10)\n",
    "print(\"Shape of train labels: {}\".format(train_labels.shape))\n",
    "print(\"First 5 train labels: \\n{}\\n\".format(train_labels[0:5]))\n",
    "\n",
    "test_labels = convert_to_one_hot(raw_test_labels,10)\n",
    "print(\"Shape of test labels: {}\".format(test_labels.shape))\n",
    "print(\"First 5 test labels: \\n{}\".format(test_labels[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data will be splitted into train and validation sets for training and validation purposes with the size of 15% of total for validation set and the rest for training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of x_train: (51000, 784)\n",
      "Length of y_train: (51000, 10)\n",
      "\n",
      "Shape of x_valid: (9000, 784)\n",
      "Shape of y_valid: (9000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(raw_converted_train_data, \n",
    "                                                    train_labels, \n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = 123)\n",
    "\n",
    "print(\"Length of x_train: \"+ str(x_train.shape))\n",
    "print(\"Length of y_train: \"+ str(y_train.shape) +\"\\n\")\n",
    "print(\"Shape of x_valid: \"+ str(x_valid.shape))\n",
    "print(\"Shape of y_valid: \"+ str(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function <span style=\"color:blue; font-family:Courier\">batch_generator</span> to split data into batched before feeding into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from Chapter 15 of the book Python for Machine Learning (Second Edition)\n",
    "def batch_generator(X, y, batch_size=64, shuffle=True, random_seed=123):\n",
    "    idx = np.arange(y.shape[0])\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic CNN structure is as follows: \n",
    "    \n",
    "    Convolution -> Pooling -> Convolution -> Pooling -> Fully Connected Layer -> Fully Connected Layer + softmax\n",
    "\n",
    "\n",
    "<img src=\"./data/images/cnn.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "\n",
    "There are three main operations in the ConvNet shown in Figure above:\n",
    "\n",
    "   - Convolution: Convolution is the act of taking the original data, and creating feature maps from it. The convolutional layers are not fully connected like a traditional neural network.\n",
    "   - Pooling or Sub Sampling: Pooling is sub-sampling, most often in the form of “max-pooling,” where we select a region, and then take the maximum value in that region, and that becomes the new value for the entire region.\n",
    "   - Classification (Fully Connected Layer): Fully Connected Layers are typical neural networks, where all nodes are “fully connected.”\n",
    "\n",
    "These operations are the basic building blocks of every Convolutional Neural Network, so understanding how these work is an important step to developing a sound understanding of ConvNets. \n",
    "\n",
    "In this architecture, we use `SAME` paddiing. Output 32 and 64 feature maps for first and second convolutional layers respectively.\n",
    "\n",
    "The dimensions of the tensors in each layer are as follows:\n",
    "    \n",
    "    - Input:                  [batchsize × 28 × 28 × 1]\n",
    "    - Conv_1:                 [batchsize × 28 × 28 × 32]\n",
    "    - Pooling_1:              [batchsize × 14 × 14 × 32]\n",
    "    - Conv_2:                 [batchsize × 14 × 14 × 64]\n",
    "    - Pooling_2:              [batchsize × 7 × 7 × 64]\n",
    "    - FC_1:                   [batchsize × 1024]\n",
    "    - FC_2 and softmax layer: [batchsize × 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "Let's define 2 functions as below for weights with random value to prevent 0 gradients and small positive bias to avoid dead neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling\n",
    "\n",
    "In this part, we will create another 2 functions for our convolution layer and max-pooling layer. Here, we will define our stride is 2, max-pooling is `2x2` with `same` padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph\n",
    "\n",
    "In this part, we will apply the architecture in the figure above to define the TensorGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    '''\n",
    "    Build TensorGraph\n",
    "    \n",
    "    Arguments:\n",
    "        None\n",
    "    Return:\n",
    "        cross_entropy, train_step, accuracy, merged: which will be used as arguments for build_cnn() function\n",
    "    '''\n",
    "    # Define Placeholders\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, 784], name = \"tf_x\")\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None, 10], name = \"tf_y\")\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name = \"fc_keep_prob\")\n",
    "\n",
    "    # Reshaped input x to 4D tensor with the second and third values are the width and height of image\n",
    "    # the last value 1 means number of color channels\n",
    "    x = tf.reshape(tf_x, [-1,28,28,1])\n",
    "    # Layer 1: First Conv Layer\n",
    "    # patch size: 5x5 ; input channel: 1 ; output channel: 32 features\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Layer 2: Second Conv Layer\n",
    "    # patch size: 5x5 ; input channel: 32 ; output channel: 64 features\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Layer 3: First Fully Connected Layer\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Drop out\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n",
    "\n",
    "    # Layer 4: Second Fully Connected Layer with softmax activation function\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    # Apply Softmax function on the 2nd FC layer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_y, logits=y_conv), \n",
    "                                   name = \"cross_entropy_loss\")\n",
    "    tf.summary.scalar(\"cost\", cross_entropy)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # train_op\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # acc\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return cross_entropy, train_step, accuracy, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training CNN model\n",
    "\n",
    "Defining function <span style=\"color:blue; font-family:Courier\">train_cnn</span> to train our CNN model. We will train data in batches and validation to test the trained model. Later we select the best model that gives us highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(sess, g, x_train, y_train, x_valid, y_valid, epoch_num, ls, ac, op, merged, initialize):\n",
    "    '''\n",
    "    Train CNN model\n",
    "    \n",
    "    Arguments:\n",
    "        sess: TensorFlow session\n",
    "        g: TensorFlow graph\n",
    "        x_train: training data\n",
    "        y_train: training labels\n",
    "        x_valid: validation data\n",
    "        y_valid: validation labels\n",
    "        epoch_num: number of training epoch\n",
    "        ls: cross_entropy_loss from the graph\n",
    "        ac: accuracy from the graph\n",
    "        op: train_step from the graph\n",
    "        merged: writer summary to display training loss and accuracy in TensorBoard\n",
    "        initialize: set to True to train model from beginning by initializing all variables\n",
    "    Return:\n",
    "        The output will be a collection trained models for every epoch\n",
    "    '''\n",
    "    dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = \"./data/tensorboard/\" + dt + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, g)\n",
    "    iteration = 1\n",
    "    \n",
    "    # If initialize is True we will start to train from beginning\n",
    "    # Else: Continue to train our selected model\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1, epoch_num+1):\n",
    "        train_loss = 0.0\n",
    "        # Start timer\n",
    "        tic = datetime.now()\n",
    "        # Create batched for training data\n",
    "        batch_train = batch_generator(x_train, y_train)\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_train):\n",
    "            # Train and calculate loss for data\n",
    "            feed = {\"tf_x:0\": batch_x, \"tf_y:0\": batch_y, \"fc_keep_prob:0\": drop_out}\n",
    "            summary, loss, _ = sess.run([merged, ls, op], feed_dict=feed)\n",
    "            # Write loss and accuracy data into TensorBoard writer\n",
    "            writer.add_summary(summary, iteration)\n",
    "            iteration +=1\n",
    "            train_loss += loss\n",
    "        # Calculate accuracy for validation data\n",
    "        feed = {\"tf_x:0\": x_valid, \"tf_y:0\": y_valid, \"fc_keep_prob:0\":1.0}\n",
    "        accuracy_valid = sess.run(ac, feed_dict=feed)\n",
    "        # Print out average train loss and accuracy for validation data\n",
    "        print(\"Epoch %02d: Avg train loss:  %7.3f | Valid acc:  %7.3f\" % (epoch, (train_loss / (i+1)), accuracy_valid))\n",
    "        # Save model every epoch\n",
    "        saver.save(sess, \"./data/model/cnn-model-{}.ckpt\".format(epoch))\n",
    "        # Stop timer\n",
    "        toc = datetime.now()\n",
    "        time = (toc - tic)\n",
    "        print(\"Time: {}\".format(time.seconds))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a graph `g` and build the graph then initiate tensor session to train the model by calling function `train_cnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Avg train loss:    0.741 | Valid acc:    0.958\n",
      "INFO:tensorflow:./data/model/cnn-model-1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 291\n",
      "Epoch 02: Avg train loss:    0.160 | Valid acc:    0.973\n",
      "INFO:tensorflow:./data/model/cnn-model-2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 291\n",
      "Epoch 03: Avg train loss:    0.110 | Valid acc:    0.978\n",
      "INFO:tensorflow:./data/model/cnn-model-3.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 273\n",
      "Epoch 04: Avg train loss:    0.083 | Valid acc:    0.982\n",
      "INFO:tensorflow:./data/model/cnn-model-4.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 284\n",
      "Epoch 05: Avg train loss:    0.065 | Valid acc:    0.982\n",
      "INFO:tensorflow:./data/model/cnn-model-5.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 286\n",
      "Epoch 06: Avg train loss:    0.055 | Valid acc:    0.985\n",
      "INFO:tensorflow:./data/model/cnn-model-6.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 277\n",
      "Epoch 07: Avg train loss:    0.046 | Valid acc:    0.987\n",
      "INFO:tensorflow:./data/model/cnn-model-7.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 285\n",
      "Epoch 08: Avg train loss:    0.040 | Valid acc:    0.987\n",
      "INFO:tensorflow:./data/model/cnn-model-8.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 275\n",
      "Epoch 09: Avg train loss:    0.034 | Valid acc:    0.988\n",
      "INFO:tensorflow:./data/model/cnn-model-9.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 285\n",
      "Epoch 10: Avg train loss:    0.029 | Valid acc:    0.989\n",
      "INFO:tensorflow:./data/model/cnn-model-10.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 278\n",
      "Epoch 11: Avg train loss:    0.025 | Valid acc:    0.990\n",
      "INFO:tensorflow:./data/model/cnn-model-11.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 310\n",
      "Epoch 12: Avg train loss:    0.023 | Valid acc:    0.987\n",
      "INFO:tensorflow:./data/model/cnn-model-12.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 291\n",
      "Epoch 13: Avg train loss:    0.019 | Valid acc:    0.989\n",
      "INFO:tensorflow:./data/model/cnn-model-13.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 271\n",
      "Epoch 14: Avg train loss:    0.016 | Valid acc:    0.990\n",
      "INFO:tensorflow:./data/model/cnn-model-14.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 269\n",
      "Epoch 15: Avg train loss:    0.017 | Valid acc:    0.989\n",
      "INFO:tensorflow:./data/model/cnn-model-15.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 267\n",
      "Epoch 16: Avg train loss:    0.013 | Valid acc:    0.990\n",
      "INFO:tensorflow:./data/model/cnn-model-16.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 279\n",
      "Epoch 17: Avg train loss:    0.012 | Valid acc:    0.989\n",
      "INFO:tensorflow:./data/model/cnn-model-17.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 279\n",
      "Epoch 18: Avg train loss:    0.011 | Valid acc:    0.991\n",
      "INFO:tensorflow:./data/model/cnn-model-18.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 284\n",
      "Epoch 19: Avg train loss:    0.011 | Valid acc:    0.990\n",
      "INFO:tensorflow:./data/model/cnn-model-19.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 277\n",
      "Epoch 20: Avg train loss:    0.009 | Valid acc:    0.989\n",
      "INFO:tensorflow:./data/model/cnn-model-20.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Time: 285\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # Build the graph\n",
    "    cross_entropy, train_step, accuracy, merged = build_cnn()\n",
    "    # Create saver\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "with tf.Session(graph = g) as sess:\n",
    "    train_cnn(sess, g, x_train, y_train, x_valid, y_valid, epoch_num, \n",
    "              cross_entropy, accuracy, train_step, merged, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training accuracy and loss via TensorBoard\n",
    "\n",
    "TensorBoard for Accuracy and the Cost during the training time after 20 epochs. \n",
    "\n",
    "<img src=\"./data/images/acc.png\" alt=\"Accuracy\" style=\"width: 65%;\"/>\n",
    "\n",
    "\n",
    "<img src=\"./data/images/cost.png\" alt=\"Cost\" style=\"width: 65%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing CNN model\n",
    "\n",
    "Let's test our trained model with our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(sess, x_test, y_test, accuracy):\n",
    "    '''\n",
    "    Predict test data\n",
    "    \n",
    "    Arguments:\n",
    "        sess: TensorFlow session\n",
    "        x_test: test data\n",
    "        y_test: test labels\n",
    "        accuracy: accuracy from the graph\n",
    "    Return:\n",
    "        Accuracy value over test set\n",
    "    '''\n",
    "    feed = {'tf_x:0': x_test,\n",
    "            \"tf_y:0\": y_test,\n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    \n",
    "    return sess.run(accuracy, feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with test set:  0.9912\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    # build the graph\n",
    "    cross_entropy, train_step, accuracy, merged = build_cnn()\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    # Restore our saved model\n",
    "    saver.restore(sess, \"./data/model/cnn-model-20.ckpt\")\n",
    "    # Use predict function to get the result\n",
    "    print(\"Accuracy with test set: \", predict(sess, raw_converted_test_data, test_labels, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to continue training our model for five more epochs and do the testing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Avg train loss:    0.008 | Valid acc:    0.990\n",
      "Time: 344\n",
      "Epoch 02: Avg train loss:    0.008 | Valid acc:    0.989\n",
      "Time: 297\n",
      "Epoch 03: Avg train loss:    0.007 | Valid acc:    0.990\n",
      "Time: 279\n",
      "Epoch 04: Avg train loss:    0.006 | Valid acc:    0.989\n",
      "Time: 297\n",
      "Epoch 05: Avg train loss:    0.006 | Valid acc:    0.991\n",
      "Time: 285\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g2) as sess:\n",
    "    # Restore our saved model\n",
    "    saver.restore(sess, \"./data/model/cnn-model-20.ckpt\")\n",
    "    # Continue to train from 20th model\n",
    "    train_cnn(sess, g2, x_train, y_train, x_valid, y_valid, 5, \n",
    "              cross_entropy, accuracy, train_step, merged, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename model, model 24 is the best 99,27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with test set:  0.9927\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g2) as sess:\n",
    "    saver.restore(sess, \"./data/model/cnn-model-24.ckpt\")\n",
    "    print(\"Accuracy with test set: \", predict(sess, raw_converted_test_data, test_labels, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners\n",
    "2. https://www.tensorflow.org/versions/r1.0/get_started/mnist/pros\n",
    "3. http://suruchifialoke.com/2017-06-17-predicting-digits-cnn-tensorflow/\n",
    "4. http://yann.lecun.com/exdb/mnist/\n",
    "5. https://colah.github.io/posts/2014-07-Conv-Nets-Modular/\n",
    "6. http://neuralnetworksanddeeplearning.com/chap6.html\n",
    "7. https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
